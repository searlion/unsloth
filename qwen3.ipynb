{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T08:10:01.299177Z",
     "start_time": "2025-08-31T08:09:53.651943Z"
    }
   },
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import weave; weave.init('think_test')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游붠 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/github/unsloth/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游붠 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1mweave\u001B[0m: Logged in as Weights & Biases user: lewis-won.\n",
      "\u001B[36m\u001B[1mweave\u001B[0m: View Weave data at https://wandb.ai/lewis-won-sear/think_test/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: lewis-won.\n",
      "View Weave data at https://wandb.ai/lewis-won-sear/think_test/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x767821ff1c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:10:36.769148Z",
     "start_time": "2025-08-31T08:10:36.767509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Model Setup Variables ---\n",
    "BASE_MODEL_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = False\n"
   ],
   "id": "85f3cde3294e01e1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:12:31.575972Z",
     "start_time": "2025-08-31T08:10:38.172119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load Model and Tokenizer ---\n",
    "BASE_MODEL, TOKENIZER = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "BASE_MODEL.eval().to(\"cuda\")\n",
    "FastLanguageModel.for_inference(BASE_MODEL)\n"
   ],
   "id": "745bbe2714acaadc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:13:15.734046Z",
     "start_time": "2025-08-31T08:13:15.731851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Prompt Preparation Functions ---\n",
    "def make_prompt(instruction):\n",
    "    return [{\"role\": \"user\", \"content\": instruction}]\n",
    "\n",
    "\n",
    "def apply_chat_template(prompt, tokenizer, enable_thinking=True):\n",
    "    messages = make_prompt(prompt)\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking,\n",
    "    )"
   ],
   "id": "1fa549d875fbce78",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:22:59.251236Z",
     "start_time": "2025-08-31T08:22:59.248905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@weave.op\n",
    "def generate_response(prompt, enable_thinking=True):\n",
    "    prompt_text = apply_chat_template(prompt, TOKENIZER, enable_thinking)\n",
    "    inputs = TOKENIZER([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        gen_output = BASE_MODEL.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            use_cache=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            top_k=20,\n",
    "            min_p=0.0,\n",
    "        )\n",
    "    output_text = TOKENIZER.decode(gen_output[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ],
   "id": "6612bbdfbba999da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:23:10.991767Z",
     "start_time": "2025-08-31T08:23:04.693708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Test Prompts ---\n",
    "math_question = \"What is 256 multiplied by 17?\"\n",
    "math_question_no_think = \"/no_think\\nWhat is 256 multiplied by 17?\"\n",
    "\n",
    "\n",
    "print(\"=== enable_thinking=True (default) ===\")\n",
    "output1 = generate_response(math_question, enable_thinking=True)\n",
    "print(output1.strip())\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"=== enable_thinking=False ===\")\n",
    "output2 = generate_response(math_question, enable_thinking=False)\n",
    "print(output2.strip())\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"=== enable_thinking=True + /no_think in prompt ===\")\n",
    "output3 = generate_response(math_question_no_think, enable_thinking=True)\n",
    "print(output3.strip())\n"
   ],
   "id": "422d4e9d42cb9126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== enable_thinking=True (default) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1mweave\u001B[0m: 游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-08fc-7fe1-8608-b6379f3e7e42\n",
      "INFO:weave.trace.weave_client:游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-08fc-7fe1-8608-b6379f3e7e42\n",
      "\u001B[36m\u001B[1mweave\u001B[0m: 游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-14fb-776d-92c0-eacaa60a1812\n",
      "INFO:weave.trace.weave_client:游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-14fb-776d-92c0-eacaa60a1812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is 256 multiplied by 17?\n",
      "assistant\n",
      "<think>\n",
      "Okay, so I need to figure out what 256 multiplied by 17 is. Hmm, let's see. I remember that multiplying numbers can be done by breaking them down into smaller parts. Maybe I can use the standard multiplication algorithm here. Let me try to remember how that works.\n",
      "\n",
      "First, let me write down the numbers to visualize them better. 256 multiplied by 17. I think 17 can be broken down into 10 and 7, right? Because 10 plus 7 equals 17. So maybe that helps me do the multiplication step by step\n",
      "\n",
      "=== enable_thinking=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1mweave\u001B[0m: 游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-1b56-7281-88d8-198a88dd6eca\n",
      "INFO:weave.trace.weave_client:游꼴 https://wandb.ai/lewis-won-sear/think_test/r/call/0198ff39-1b56-7281-88d8-198a88dd6eca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is 256 multiplied by 17?\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "To calculate **256 칑 17**, we can break it down:\n",
      "\n",
      "$$\n",
      "256 \\times 17 = 256 \\times (10 + 7) = 256 \\times 10 + 256 \\times 7\n",
      "$$\n",
      "\n",
      "$$\n",
      "= 2560 + 1792 = 4352\n",
      "$$\n",
      "\n",
      "So, **256 칑 17 = 4352**.\n",
      "\n",
      "=== enable_thinking=True + /no_think in prompt ===\n",
      "user\n",
      "/no_think\n",
      "What is 256 multiplied by 17?\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "To find the product of 256 and 17:\n",
      "\n",
      "$$\n",
      "256 \\times 17 = 256 \\times (10 + 7) = 256 \\times 10 + 256 \\times 7 = 2560 + 1792 = 4352\n",
      "$$\n",
      "\n",
      "So, **256 multiplied by 17 is 4352**.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:26:22.072558Z",
     "start_time": "2025-08-31T08:26:22.069721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "SEED = 3407\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ],
   "id": "96148994cc815952",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:26:32.324133Z",
     "start_time": "2025-08-31T08:26:32.322196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ====== REST OF SCRIPT ======\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = False\n",
    "MODEL_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "SAVE_DIR = \"lora_model\"\n"
   ],
   "id": "333cd62eca2e8d13",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:26:50.255080Z",
     "start_time": "2025-08-31T08:26:40.921114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n"
   ],
   "id": "136c359f7733fe04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.10 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:26:52.831539Z",
     "start_time": "2025-08-31T08:26:52.829116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        if input_text.strip():\n",
    "            user_message = f\"{instruction}\\n\\n{input_text}\"\n",
    "        else:\n",
    "            user_message = instruction\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": output},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n"
   ],
   "id": "c81e7eb3ed29d14c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:27:08.457174Z",
     "start_time": "2025-08-31T08:27:00.314128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "half_len = len(dataset) // 2\n",
    "dataset = dataset.select(range(half_len))\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=2)\n"
   ],
   "id": "35857dff6c905aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 51760/51760 [00:00<00:00, 242706.33 examples/s]\n",
      "Map (num_proc=2): 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 25880/25880 [00:00<00:00, 33469.49 examples/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:27:50.531026Z",
     "start_time": "2025-08-31T08:27:11.459446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=SEED,  # Make sure to set this!\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n"
   ],
   "id": "658b212b13181319",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=16): 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 25880/25880 [00:02<00:00, 11114.12 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 25,880 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544 of 606,142,464 (1.67% trained)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mlewis-won\u001B[0m (\u001B[33mlewis-won-sear\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "creating run (0.1s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/lewis/github/unsloth/wandb/run-20250831_162716-sz4l72ii</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lewis-won-sear/huggingface/runs/sz4l72ii' target=\"_blank\">earthy-snowball-1</a></strong> to <a href='https://wandb.ai/lewis-won-sear/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/lewis-won-sear/huggingface' target=\"_blank\">https://wandb.ai/lewis-won-sear/huggingface</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/lewis-won-sear/huggingface/runs/sz4l72ii' target=\"_blank\">https://wandb.ai/lewis-won-sear/huggingface/runs/sz4l72ii</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Initializing weave.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "/home/lewis/github/unsloth/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/lewis/github/unsloth/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1mweave\u001B[0m: Logged in as Weights & Biases user: lewis-won.\n",
      "\u001B[36m\u001B[1mweave\u001B[0m: View Weave data at https://wandb.ai/lewis-won-sear/huggingface/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: lewis-won.\n",
      "View Weave data at https://wandb.ai/lewis-won-sear/huggingface/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.082800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.313800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.426100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.680800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.227000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.779800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.866300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.835800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.946300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.999100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.801700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.755500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.606700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.788000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.668800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.595400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.462000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.226300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.325400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.662500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.566400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.834100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.214800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.508700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.497600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.464200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.771000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.655600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.528400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.306100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.239400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.485800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.494200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.481900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.375600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.522200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.754800</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.241200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.391300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.772400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.350900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.761500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.559700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.507100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.483900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.666200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.517900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.576700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.789300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.395100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.174400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.483500</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.304700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.353400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.426300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.474400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.491600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.559000</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.522900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.6049276173114777, metrics={'train_runtime': 34.866, 'train_samples_per_second': 13.767, 'train_steps_per_second': 1.721, 'total_flos': 327517470720000.0, 'train_loss': 1.6049276173114777, 'epoch': 0.01854714064914992})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:32:31.358233Z",
     "start_time": "2025-08-31T08:32:31.347609Z"
    }
   },
   "cell_type": "code",
   "source": "FastLanguageModel.for_inference(model)",
   "id": "202fd6fa65ede744",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:32:42.776105Z",
     "start_time": "2025-08-31T08:32:41.127429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_query = \"Continue the Fibonacci sequence.\\n\\n1, 1, 2, 3, 5, 8\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    use_cache=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=========== Output from in-memory model (just trained):\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "id": "fa99f569a866cef2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Output from in-memory model (just trained):\n",
      "user\n",
      "Continue the Fibonacci sequence.\n",
      "\n",
      "1, 1, 2, 3, 5, 8\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:32:54.847552Z",
     "start_time": "2025-08-31T08:32:54.132228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n"
   ],
   "id": "5bbf3728f45353cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:34:18.563988Z",
     "start_time": "2025-08-31T08:33:06.125952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "bff1d991d2966957",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Input IDs of shape torch.Size([1, 2049]) with length 2049 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2050]) with length 2050 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2051]) with length 2051 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2052]) with length 2052 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2053]) with length 2053 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2054]) with length 2054 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2055]) with length 2055 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2056]) with length 2056 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2057]) with length 2057 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2058]) with length 2058 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2059]) with length 2059 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2060]) with length 2060 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2061]) with length 2061 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2062]) with length 2062 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2063]) with length 2063 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2064]) with length 2064 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2065]) with length 2065 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2066]) with length 2066 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2067]) with length 2067 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2068]) with length 2068 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2069]) with length 2069 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2070]) with length 2070 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2071]) with length 2071 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2072]) with length 2072 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2073]) with length 2073 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2074]) with length 2074 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2075]) with length 2075 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2076]) with length 2076 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2077]) with length 2077 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2078]) with length 2078 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2079]) with length 2079 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of shape torch.Size([1, 2080]) with length 2080 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Output from reloaded model (after save/load):\n",
      "user\n",
      "Continue the Fibonacci sequence.\n",
      "\n",
      "1, 1, 2, 3, 5, 8\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The Fibonacci sequence is defined as follows:\n",
      "\n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049, 12586269025, 20365011074, 32951280109, 53316291183, 86267571292, 139583862475, 225851433767, 365435306242, 591286739909, 956722046151, 1548008786060, 2504730832211, 4052739618271, 6557470450482, 10610209068753, 17167689519235, 27777898587988, 44945588107223, 72723486695211, 117679074702434, 190402561397645, 308081636100079, 508484207497724, 816565843597803, 1325049050095527, 2141614893693320, 3466663943788847, 5608288837482167, 9074952781271014, 14683241618753181, 23758194400024195, 38441436018777376, 62209630418801571, 100651066437578947, 162860696856380518, 263411763293959465, 426272450150339983, 689684213444339448, 1115956663694679431, 1805640877138918879, 2921597540833598310, 4727238417972517189, 7648835958806115499, 12376074376778632688, 20024910335584748187, 32400984712363380875, 52425895047948129062, 84826879760311509937, 137252774808259638999, 222079654568571148936, 359332429376830788935, 581412083945401937871, 940744513322232726806, 1522156597267634664677, 2462891110599867391483, 3985047707867402056160, 6447938818467269447643, 10433086526334671403803, 16881025344801940851446, 27314111871136612255249, 44195137216038553106695, 71509249087175165361944, 115704386303213718468639, 187213635390388883830683, 302918021693602502299322, 490131657083991386129905, 803049678777593888429227, 1293181335861585274619132, 2106231014639179162048359, 3400412350500764436667491, 5506643365139943608715850, 8907055715640708045383341, 14413699080780651654109291, 23320754796421359699492632, 37734453877202011353601923, 61055208673623371053094555, 98789662550825382406706478, 160844871224448853469801033, 259634533775274235876507511, 42047999999999999999999999999999999999\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:35:52.829255Z",
     "start_time": "2025-08-31T08:35:45.805367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=SAVE_DIR,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "prompt2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "inputs2 = tokenizer([prompt2], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs2 = model.generate(\n",
    "    **inputs2,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=========== Output from reloaded model (after save/load):\")\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True))"
   ],
   "id": "c9fe7b6911ac7790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "=========== Output from reloaded model (after save/load):\n",
      "user\n",
      "Continue the Fibonacci sequence.\n",
      "\n",
      "1, 1, 2, 3, 5, 8\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The next Fibonacci numbers after 1, 1, 2, 3, 5, 8 would be 13, 21, 34, 55, and 89.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:37:07.795631Z",
     "start_time": "2025-08-31T08:37:06.586219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "SEED = 3407\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "import unsloth\n",
    "from datasets import load_dataset\n",
    "import weave\n",
    "import asyncio\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = False\n",
    "BASE_MODEL_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "LORA_MODEL_DIR = \"lora_model\"\n",
    "N = 30\n",
    "\n",
    "\n",
    "weave.init(\"huggingface\")\n"
   ],
   "id": "c05eecf7e3579c30",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1mweave\u001B[0m: Logged in as Weights & Biases user: lewis-won.\n",
      "\u001B[36m\u001B[1mweave\u001B[0m: View Weave data at https://wandb.ai/lewis-won-sear/huggingface/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: lewis-won.\n",
      "View Weave data at https://wandb.ai/lewis-won-sear/huggingface/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x76772419fbc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T08:37:26.826958Z",
     "start_time": "2025-08-31T08:37:14.837728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === GLOBAL: LOAD MODELS ONLY ONCE ===\n",
    "BASE_MODEL, TOKENIZER = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "LORA_MODEL, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=LORA_MODEL_DIR,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "BASE_MODEL.eval().to(\"cuda\")\n",
    "LORA_MODEL.eval().to(\"cuda\")\n",
    "FastLanguageModel.for_inference(BASE_MODEL)\n",
    "FastLanguageModel.for_inference(LORA_MODEL)\n"
   ],
   "id": "4bc50ec193124bc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.713 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
